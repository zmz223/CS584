{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "occasional-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import subprocess\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fitting-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts valid split to image paths\n",
    "valid_in = open(\"./TrainVal/VOCdevkit/VOC2011/ImageSets/Segmentation/val.txt\", \"r\")\n",
    "valid_out = open(\"./valid.txt\", \"w\")\n",
    "for line in valid_in:\n",
    "    valid_out.write(\"../TrainVal/VOCdevkit/VOC2011/JPEGImages/\" + line[:-1] + \".jpg\\n\")\n",
    "valid_in.close()\n",
    "valid_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "greenhouse-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs shell script to install darknet files and run inference...takes a long time so the txt are submitted too\n",
    "#This step is only included for reproducibility\n",
    "# subprocess.run([\"chmod\", \"+x\", \"install-dependencies.sh\"])\n",
    "# proc = subprocess.run(\"./install-dependencies.sh\",  capture_output=True, shell=True)\n",
    "# print(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stunning-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Builds dataframe for use throughout the program\n",
    "def build_df(annot_path, image_path, split_path):    \n",
    "    data = pd.DataFrame(columns=[\"image\", \"annotation\"])\n",
    "    split = open(split_path, \"r\")\n",
    "    lines = split.readlines()\n",
    "    split.close()\n",
    "    jpg = []\n",
    "    xml = []\n",
    "    for i in lines:\n",
    "        temp = i.split(\" \")[0][:-1]\n",
    "        jpg.append(image_path+temp+\".jpg\")\n",
    "        xml.append(annot_path + temp + \".xml\")\n",
    "    data[\"image\"] = jpg\n",
    "    data[\"annotation\"] = xml\n",
    "    return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "still-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract coordinates, labels, and bounding boxes from yolo predictions\n",
    "#Data is the datframe\n",
    "#File path is the file path oif the output of the models predictions\n",
    "#Start index can either be 4 or 7 depending on the model (4 -> tiny, 6-> standard)\n",
    "def extraction(data, file_path, start_ind):\n",
    "    assert start_ind == 4 or start_ind == 6\n",
    "    idx = 0\n",
    "    images = OrderedDict()\n",
    "    for path in data[\"image\"]:\n",
    "        images[path] = []\n",
    "    fp = None\n",
    "    times = []\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[start_ind:-1]:\n",
    "            split = line.split()\n",
    "            if split[0] == \"Enter\":\n",
    "                fp = data[\"image\"].iloc[idx]\n",
    "                idx+=1\n",
    "            elif split[0][0] == \".\":\n",
    "                times.append(split[3])\n",
    "            else:\n",
    "                box = [split[0], split[1][:-1], split[3], split[5], split[7], split[9][:-1]]\n",
    "                images[fp].append(box)\n",
    "    return images, times\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "twenty-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parses through XML annotations and reads bb information\n",
    "def find_boxes(file):\n",
    "\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = []\n",
    "    \n",
    "    for box in root.iter('object'):\n",
    "        name = str(box.find(\"name\").text)\n",
    "        ymin = int(box.find(\"bndbox/ymin\").text)\n",
    "        xmin = int(box.find(\"bndbox/xmin\").text)\n",
    "        ymax = int(box.find(\"bndbox/ymax\").text)\n",
    "        xmax = int(box.find(\"bndbox/xmax\").text)\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        boxes.append([name, xmin, ymin, width, height])\n",
    "        \n",
    "    return boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "weighted-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the annotations to RAM using the above function\n",
    "def import_annotations(data):\n",
    "    counter = 0\n",
    "    annotations = OrderedDict()\n",
    "    for file in data[\"annotation\"]:\n",
    "        boxes = find_boxes(file)\n",
    "        annotations[data[\"image\"].iloc[counter]] = boxes\n",
    "        counter+=1\n",
    "    return annotations\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "related-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def IoU(gt, pred):\n",
    "    gt_x, gt_y, gt_w, gt_h = gt[1:]\n",
    "    \n",
    "    pred_x, pred_y, pred_w, pred_h = pred[2:]\n",
    "    \n",
    "    x_left = max(gt_x, pred_x)\n",
    "    y_top = max(gt_y, pred_y)\n",
    "    x_right = min((gt_x + gt_w), (pred_x + pred_w))\n",
    "    y_bottom = min((gt_y+gt_h), (pred_y+pred_h))\n",
    "    \n",
    "    intersect = (x_right - x_left) * (y_bottom - y_top)\n",
    "    ground_area = gt_w * gt_h\n",
    "    pred_area = pred_w * pred_h\n",
    "    iou = intersect / (ground_area + pred_area - intersect)\n",
    "    return iou\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "handed-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mAP_helper(gt, pred):\n",
    "    output = None\n",
    "    iou = IoU(gt, pred)\n",
    "    if (iou >= 0.5 and gt[i][0] == None) or (iou >= 0.5 and gt[i][0] != pred[i][0]):\n",
    "        output = \"FN\"\n",
    "    elif iou >= 0.5:\n",
    "        output = \"TP\"\n",
    "    else:\n",
    "        output = \"FP\"\n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adult-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiny_yolo():\n",
    "    annot_path = \"./TrainVal/VOCdevkit/VOC2011/Annotations/\"\n",
    "    image_path = \"./TrainVal/VOCdevkit/VOC2011/JPEGImages/\"\n",
    "    split_path = \"./TrainVal/VOCdevkit/VOC2011/ImageSets/Segmentation/val.txt\"\n",
    "    data = build_df(annot_path, image_path, split_path)\n",
    "    predictions, times = extraction(data, \"tiny_results.txt\", 4)\n",
    "    ground_truth = import_annotations(data)\n",
    "    conf_thresh = 70\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    with open(\"tiny_yolo_metrics.txt\", \"w\") as f:\n",
    "        for pred in predictions:\n",
    "            f.write(\"Image Path: \" + pred)\n",
    "            f.write(\"\\t\\tTime for predictions: \" + str(times[counter]) + \"\\n\")\n",
    "            counter+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "later-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_yolo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "assured-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_yolo():\n",
    "    annot_path = \"./TrainVal/VOCdevkit/VOC2011/Annotations/\"\n",
    "    image_path = \"./TrainVal/VOCdevkit/VOC2011/JPEGImages/\"\n",
    "    split_path = \"./TrainVal/VOCdevkit/VOC2011/ImageSets/Segmentation/val.txt\"\n",
    "    data = build_df(annot_path, image_path, split_path)\n",
    "    predictions, times = extraction(data, \"standard_results.txt\", 6)\n",
    "    ground_truth = import_annotations(data)\n",
    "    conf_thresh = 70\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    with open(\"standard_yolo_metrics.txt\", \"w\") as f:\n",
    "        for pred in predictions:\n",
    "            f.write(\"Image Path: \" + pred)\n",
    "            f.write(\"\\t\\tTime for predictions: \" + str(times[counter]) + \"\\n\")\n",
    "            counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "swedish-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_yolo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-adapter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
