{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note: Couldn't quite finish the assignment by the deadline. \n",
    "I had some trouble loading the model and didn't resolve the issue until earlier today. \n",
    "I have everything working up to mAP calculation. So far, the mAP function calculates the number of TP, FP, and FN \n",
    "in the validation split\n",
    "\n",
    "\n",
    "Summary of included files\n",
    "1. tiny_results.txt -> results of inference using tiny-yolov2\n",
    "2. tiny_yolo_metrics.txt -> parsed tiny_results.txt giving runtime of the tiny-yolo on each image\n",
    "3. standard_results.txt -> results of inference using yolov2\n",
    "4. standard_yolo_metrics.txt -> parsed standard_results.txt giving runtime of the tiny-yolo on each image\n",
    "5. install-dependencies.sh -> shell script to install darknet, compile the model, and run inference on the validation\n",
    "    split of the VOC data\n",
    "6. hw5.ipynb -> Notebook with all code accomplishing the specified tasks\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import subprocess\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts valid split to image paths\n",
    "valid_in = open(\"./TrainVal/VOCdevkit/VOC2011/ImageSets/Segmentation/val.txt\", \"r\")\n",
    "valid_out = open(\"./valid.txt\", \"w\")\n",
    "for line in valid_in:\n",
    "    valid_out.write(\"../TrainVal/VOCdevkit/VOC2011/JPEGImages/\" + line[:-1] + \".jpg\\n\")\n",
    "valid_in.close()\n",
    "valid_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs shell script to install darknet files and run inference...takes a long time so the txt are submitted too\n",
    "#This step is only included for reproducibility\n",
    "# subprocess.run([\"chmod\", \"+x\", \"install-dependencies.sh\"])\n",
    "# proc = subprocess.run(\"./install-dependencies.sh\",  capture_output=True, shell=True)\n",
    "# print(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Builds dataframe for use throughout the program\n",
    "def build_df(annot_path, image_path, split_path):    \n",
    "    data = pd.DataFrame(columns=[\"image\", \"annotation\"])\n",
    "    split = open(split_path, \"r\")\n",
    "    lines = split.readlines()\n",
    "    split.close()\n",
    "    jpg = []\n",
    "    xml = []\n",
    "    for i in lines:\n",
    "        temp = i.split(\" \")[0][:-1]\n",
    "        jpg.append(image_path+temp+\".jpg\")\n",
    "        xml.append(annot_path + temp + \".xml\")\n",
    "    data[\"image\"] = jpg\n",
    "    data[\"annotation\"] = xml\n",
    "    return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract coordinates, labels, and bounding boxes from yolo predictions\n",
    "#Data is the datframe\n",
    "#File path is the file path oif the output of the models predictions\n",
    "#Start index can either be 4 or 7 depending on the model (4 -> tiny, 6-> standard)\n",
    "def extraction(data, file_path, start_ind):\n",
    "    assert start_ind == 4 or start_ind == 6\n",
    "    idx = 0\n",
    "    images = OrderedDict()\n",
    "    for path in data[\"image\"]:\n",
    "        images[path] = []\n",
    "    fp = None\n",
    "    times = []\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[start_ind:-1]:\n",
    "            split = line.split()\n",
    "            if split[0] == \"Enter\":\n",
    "                fp = data[\"image\"].iloc[idx]\n",
    "                idx+=1\n",
    "            elif split[0][0] == \".\":\n",
    "                times.append(split[3])\n",
    "            else:\n",
    "                box = [split[0], split[1][:-1], split[3], split[5], split[7], split[9][:-1]]\n",
    "                images[fp].append(box)\n",
    "    return images, times\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parses through XML annotations and reads bb information\n",
    "def find_boxes(file):\n",
    "\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = []\n",
    "    \n",
    "    for box in root.iter('object'):\n",
    "        name = str(box.find(\"name\").text)\n",
    "        ymin = int(box.find(\"bndbox/ymin\").text)\n",
    "        xmin = int(box.find(\"bndbox/xmin\").text)\n",
    "        ymax = int(box.find(\"bndbox/ymax\").text)\n",
    "        xmax = int(box.find(\"bndbox/xmax\").text)\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        boxes.append([name, xmin, ymin, width, height])\n",
    "        \n",
    "    return boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the annotations to RAM using the above function\n",
    "def import_annotations(data):\n",
    "    counter = 0\n",
    "    annotations = OrderedDict()\n",
    "    for file in data[\"annotation\"]:\n",
    "        boxes = find_boxes(file)\n",
    "        annotations[data[\"image\"].iloc[counter]] = boxes\n",
    "        counter+=1\n",
    "    return annotations\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the IoU for a single prediction and ground truth annotation\n",
    "def IoU(gt, pred):\n",
    "    gt_x, gt_y, gt_w, gt_h = gt[1:]\n",
    "    \n",
    "    pred_x, pred_y, pred_w, pred_h = pred[2:]\n",
    "    \n",
    "    x_left = max(int(gt_x), int(pred_x))\n",
    "    y_top = max(int(gt_y), int(pred_y))\n",
    "    x_right = min((int(gt_x) + int(gt_w)), (int(pred_x) + int(pred_w)))\n",
    "    y_bottom = min((int(gt_y)+int(gt_h)), (int(pred_y)+int(pred_h)))\n",
    "    \n",
    "    intersect = (x_right - x_left) * (y_bottom - y_top)\n",
    "    ground_area = int(gt_w) * int(gt_h)\n",
    "    pred_area = int(pred_w) * int(pred_h)\n",
    "    denom = (ground_area + pred_area - intersect)\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        iou = intersect / (ground_area + pred_area - intersect)\n",
    "    return iou\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates IOU within certain threshold and confidence and returns the counts of TP, FP, and FN by class in a dictionary\n",
    "def mAP(ground_truth, predictions, conf_thresh):\n",
    "    confusion = dict()\n",
    "    for gt in ground_truth:\n",
    "        for pred in predictions:\n",
    "            for inst in ground_truth[gt]:\n",
    "                for inst_pred in predictions[pred]:\n",
    "                    if int(inst_pred[1]) >= conf_thresh:\n",
    "                        iou = IoU(inst, inst_pred)\n",
    "                        classi = \"FP\"\n",
    "                        if (iou >= 0.5 and inst_pred[0][:-1] == None) or (iou >= 0.5 and inst_pred[0][:-1] == inst[0]):\n",
    "                            classi = \"FN\"\n",
    "                        elif iou >= 0.5:\n",
    "                            classi = \"TP\"\n",
    "                        if inst_pred[0][:-1] not in confusion:\n",
    "                            confusion[inst_pred[0][:-1]] = [classi]\n",
    "                        else:\n",
    "                            confusion[inst_pred[0][:-1]].append(classi)\n",
    "    return confusion            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writes output time and mAP to a file for tiny-yolo\n",
    "def tiny_yolo():\n",
    "    annot_path = \"./TrainVal/VOCdevkit/VOC2011/Annotations/\"\n",
    "    image_path = \"./TrainVal/VOCdevkit/VOC2011/JPEGImages/\"\n",
    "    split_path = \"./TrainVal/VOCdevkit/VOC2011/ImageSets/Segmentation/val.txt\"\n",
    "    data = build_df(annot_path, image_path, split_path)\n",
    "    predictions, times = extraction(data, \"tiny_results.txt\", 4)\n",
    "    ground_truth = import_annotations(data)\n",
    "    conf_thresh = 70\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    with open(\"tiny_yolo_metrics.txt\", \"w\") as f:\n",
    "        for pred in predictions:\n",
    "            f.write(\"Image Path: \" + pred)\n",
    "            f.write(\"\\t\\tTime for predictions: \" + str(times[counter]) + \"\\n\")\n",
    "            counter+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_yolo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writes output time and mAP to a file for standard yolo\n",
    "def standard_yolo():\n",
    "    annot_path = \"./TrainVal/VOCdevkit/VOC2011/Annotations/\"\n",
    "    image_path = \"./TrainVal/VOCdevkit/VOC2011/JPEGImages/\"\n",
    "    split_path = \"./TrainVal/VOCdevkit/VOC2011/ImageSets/Segmentation/val.txt\"\n",
    "    data = build_df(annot_path, image_path, split_path)\n",
    "    predictions, times = extraction(data, \"standard_results.txt\", 6)\n",
    "    ground_truth = import_annotations(data)\n",
    "    conf_thresh = 70\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    with open(\"standard_yolo_metrics.txt\", \"w\") as f:\n",
    "        for pred in predictions:\n",
    "            f.write(\"Image Path: \" + pred)\n",
    "            f.write(\"\\t\\tTime for predictions: \" + str(times[counter]) + \"\\n\")\n",
    "            counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_yolo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-plane",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
